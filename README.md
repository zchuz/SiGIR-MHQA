# Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering

This repository contains the code, model, and data for the ACL 2025 Findings paper: *Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering*

## Requirements
The environment is derived from [LlamaFactory v0.8.1](https://github.com/hiyouga/LLaMA-Factory/tree/v0.8.1). 
Please refer to the repository for installation of the dependencies.

```
conda create -n llama_factory_810 python=3.10
conda activate llama_factory_810

git clone -b v0.8.1 --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```

## Retrieval Setup

### Retrieval Corpus
We use the contexts (supporting and irrelevant documents) from training set of 2WikiMQA, HotpotQA, and MuSiQue to construct the retrieval corpus for each dataset, respectively.

You can download ours pre-processed [retrieval_corpus]().


### Sparse Retrieval
We use BM25 supported by Elastic Search for sparse retrieval.
Please refer to the Elastic Search [official documentation](https://www.elastic.co/docs/deploy-manage/deploy/self-managed/install-elasticsearch-from-archive-on-linux-macos) for installation.
We use Elastic Search 8.1.3.

First launch Elastic Search
```bash
./your_path_to_elasticsearch/bin/elasticsearch &
```

First time running, you need to build the index first.
```python 
python ./retrieval/bm25/build_index/index_2wiki_wiki.py
python ./retrieval/bm25/build_index/index_hotpotqa_wiki.py
python ./retrieval/bm25/build_index/index_musique_wiki.py
```

Start the query service
```python
python ./retrieval/bm25/run_index/run_2wiki_index.py &
python ./retrieval/bm25/run_index/run_hotpotqa_index.py &
python ./retrieval/bm25/run_index/run_musique_index.py &
```


### Dense Retrieval
We use [Contriever-msmarco](https://huggingface.co/facebook/contriever-msmarco) as default dense retrieval encoder.

For Contriever-msmarco, 
```python
python -m retrieval.contriever.retrieval_api
```

For BGE-large,
```python
python -m retrieval.bge_large.retrieval_api
```


### Hybrid Retrieval
We combine the results of sparse retrieval and dense retrieval as the hybrid retrieval result.

## Data Synthesis Pipeline

1. Base Data Generation: 
We first use DeepSeek-V2.5 to conduct reasoning on a portion of the training data (corpus $X$), then parse the results to obtain iterative reasoning trajectories with sub-question decomposition.
```python
./data_generation/ours/base_data
```

2. Naive Reasoner Training:
We use the aforementioned reasoning trajectories to train a naive reasoner $R$.
```python
./train/sft.py
```

3. Critic Model Training:
We first use the naive reasoner to inference on a corpus disjoint from the training set (corpus $X_{critic}$). 
We then reformat this data into the critique task format and annotate it using DeepSeek-V2.5. 
Finally, we train a set of critique models $C$ based on this processed data.
```python
./data_generation/ours/critic
./train/sft_critic.py
```

4. Self-Critique Reasoner Training:
We use the critique models to annotate the iterative reasoning data, resulting in reasoning trajectories with interleaved reasoning and self-critique signals.
Then, we train a self-critique reasoner $R_{sc}$ using these data.
```python
./data_generation/ours/generator
./train/sft.py
```

We provide already pre-processed data here:
- Base reasoning trajectories generated by DeepSeek-V2.5 [download link]().
- Reasoning trajectories with interleaved self-critique and reasoning steps [download link]().


## Training

We use 4xA100-80G, and train the model using LoRA.

Please refer to the training script.
```bash
./train/scripts/sft.sh
```

## Evaluation

We use for [vLLM-0.5.0.post1](https://github.com/vllm-project/vllm/tree/v0.5.0.post1) for faster inference.

Please refer to the inference script.
```bash
./inference/script/inference.sh
```

You can switch different inference functions in the evaluation script to implement different evaluation methods.
```bash
--inference_func [vanilla/greedy/guided]
```





