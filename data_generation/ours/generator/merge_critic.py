# create date: 2024/12/18
# conda env: vllm
# This file is used to merge the critic data into the training data.

import re
import os
import json
from tqdm.notebook import tqdm
from collections import defaultdict, Counter

import sys
sys.path.append("/home/zchu/codes/train_2412")
from data_generation.utils import f1_score, em_score, pm_score

def extract_answer(text):
    pattern = r"@@(.*?)@@"
    match = re.search(pattern, text)
    if match:
        return match.group(1)
    return "Unknown"

def load_json(fp):
    return json.load(open(fp, 'r'))

def load_jsonl(fp):
    return [json.loads(line) for line in open(fp, 'r')]

def process(data_list, reasoning):
    return_list = []
    for data in data_list:
        instruction = data["instruction"]
        rating = data["output"]["rating"]
        question = re.findall(r"### Question:(.*?)### Evidence", instruction, re.DOTALL)[0].strip()
        ques_position = reasoning.find(question)
        tmp = {
            "question": question,
            "rating": rating,
            "position": ques_position,
            "instruction": instruction,
            "meta": data["meta"]
        }
        return_list.append(tmp)
    return sorted(return_list, key=lambda x: x["position"])

# fp_raw_data: the raw data generated by 1-st round trained generator model.
# fp_merged_data: the merged data generated by critic model.

FP_RAW_DATA = "/home/zchu/codes/train_2412/inference/outputs/t2_train/0_None/results.json"
FP_MERGED_DATA = "/home/zchu/codes/train_2412/data_generation/ours_T2/critic/outputs/merged_results.json"
if __name__ == "__main__":
    raw_datas = load_json(FP_RAW_DATA)
    raw_datas_map = {item["qid"]: item for item in raw_datas}
    merged_results = load_json(FP_MERGED_DATA)
    ground_results = [item for item in merged_results if item["meta"]["task"] == "groundness"]
    relevance_results = [item for item in merged_results if item["meta"]["task"] == "relevance"]
    utility_results = [item for item in merged_results if item["meta"]["task"] == "utility"]
    
    ground_data_map = defaultdict(list)
    relevance_data_map = defaultdict(list)
    utility_data_map = defaultdict(list)

    for item in ground_results:
        qid = item["meta"]["qid"]
        ground_data_map[qid].append(item)

    for item in relevance_results:
        qid = item["meta"]["qid"]
        relevance_data_map[qid].append(item)

    for item in utility_results:
        qid = item["meta"]["qid"]
        utility_data_map[qid].append(item)
    
    error_keys = []
    response_text_list = []
    prompt_text_list = []
    n_valid = 0
    ground_data_list, relevance_data_list = [], []
    training_examples = []
    for key in tqdm(raw_datas_map):
        data = raw_datas_map[key]
        ground_truth = data["answer"]
        prediction = extract_answer(data["reasoning"])
        em, pm, f1 = em_score(ground_truth, prediction), pm_score(ground_truth, prediction), f1_score(ground_truth, prediction)[0]
        meta = {
            "dataset": data["dataset"],
            "type": data["type"],
            "id": data["qid"],
            "metrics": {
                "em": em,
                "pm": pm,
                "f1": f1
            }
        }
        if key not in ground_data_map or key not in relevance_data_map:
            continue
        ground_data = ground_data_map[key]
        relevance_data = relevance_data_map[key]
        utility_data = utility_data_map[key]
        
        if len(ground_data) != len(relevance_data):
            error_keys.append(key)
            continue
        
        if len(ground_data) != len(data["documents"]):
            error_keys.append(key)
            continue
        meta.update({"rule_based_rating": utility_data[0]["rule_based_rating"]})
        utility_data_rating = utility_data[0]["output"]["rating"]
        ground_data = process(ground_data, data["reasoning"])
        relevance_data = process(relevance_data, data["reasoning"])
        ground_data_list.append(ground_data)
        relevance_data_list.append(relevance_data)
        document_list = [f"{item['title']}: {item['paragraph_text']}" for item in data["documents"]]
        completion_list = data["completion_list"]
        
        prompt_text = re.findall(r"\[INST\](.*?)\[\/INST\]", data["reasoning"], re.DOTALL)[0].strip()
        response_text = ""
        
        for i in range(len(document_list)):
            # add subquestion
            response_text += completion_list[2*i].lstrip()
            # add retrieval document
            document = document_list[i]
            response_text = response_text + document + "</paragraph>" + relevance_data[i]["rating"]
            # add reasoning
            if completion_list[2*i+1].endswith("\n\n"):
                response_text = response_text + completion_list[2*i+1].strip() + ground_data[i]["rating"] + "\n\n"
            else:
                response_text = response_text + completion_list[2*i+1].lstrip() + ground_data[i]["rating"]
        response_text += completion_list[-1]
        response_text = response_text.replace("</s>", utility_data_rating)
        response_text = response_text.replace("  ", " ")
        #response_text = response_text.replace("</s>", f"{utility_data_rating}</s>").replace("  ", " ")
        
        prompt_text_list.append(prompt_text)
        response_text_list.append(response_text)
        training_examples.append({
            "instruction": prompt_text,
            "output": response_text,
            "meta": meta
        })

    with open("/home/zchu/codes/train_2412/dataset/train.json", "w") as f:
        json.dump(training_examples, f, ensure_ascii=False, indent=4)
